{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对测试数据进行记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Base-model在六个领域上的基线\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('E:\\学习\\python\\py_codbase\\PK_LLM')\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset \n",
    "from ragas.metrics import answer_relevancy,answer_correctness,answer_correctness\n",
    "from ragas import evaluate\n",
    "data_path = \"eval_reasult\"\n",
    "# gpt-4 ; gpt-3.5-turbo ; glm-3-turbo ; glm-4 ; rag\n",
    "\n",
    "def eval_ar(q_lis:list,a_lis:list,t_lis):\n",
    "    data_samples = {\n",
    "        'question':q_lis,\n",
    "        'answer':a_lis,\n",
    "        'contexts':[[\"\"] for i in range(len(q_lis))],\n",
    "        'ground_truth':t_lis\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data_samples)\n",
    "    score = evaluate(dataset,metrics=[answer_relevancy,answer_correctness])\n",
    "    df = score.to_pandas()\n",
    "    return df['answer_relevancy'].mean(),df['answer_correctness'].mean()\n",
    "\n",
    "def process_Di(mt,data:list,_i = 1)->dict:\n",
    "    # 返回模型的AC和AR\n",
    "    dir ={f\"AC{_i}\":[],f'AR{_i}':[]}\n",
    "    path = f\"{data_path}\\\\{_i}\"\n",
    "    t_lis = [a['Answer'] for a in data]\n",
    "    for _m in mt:\n",
    "        m_data = f\"{path}\\eval_{_m}.csv\"\n",
    "        if _m == 'rag':\n",
    "            m_data = \"eval_reasult\\\\1\\\\new_eval_rag.csv\"\n",
    "        m_df = pd.read_csv(m_data).dropna(subset=['prediction'],axis=0)\n",
    "        # _ac = m_df['answer_correctness'].mean()\n",
    "        _ar,_ac = eval_ar(a_lis=list(m_df['prediction']),\n",
    "                        q_lis=list(m_df['Question']),\n",
    "                        t_lis=t_lis)\n",
    "        print(f\"{_m} 的平均相关性为 {_ar}\")\n",
    "        dir[f'AC{_i}'].append(_ac)\n",
    "        print(f\"{_m} 的平均正确率为 {_ac}\")\n",
    "        dir[f'AR{_i}'].append(_ar)\n",
    "    return dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PK_LLM_endfront.evaluation.eva_funcs import DataLoader\n",
    "import time\n",
    "mt = ['glm-3-turbo','gpt-4','gpt-3.5-turbo','rag']#'glm-4'\n",
    "d_lis = []\n",
    "out_df = pd.DataFrame([],index=mt,columns=d_lis)\n",
    "for i in range(1,7):\n",
    "    out_df = pd.read_csv(\"eval_reasult2\\\\basellm_score.csv\",index_col=0)\n",
    "    if i <2:\n",
    "        continue\n",
    "    print(out_df.shape)\n",
    "    data = DataLoader.load_json(f\"_data\\\\testqa2\\\\testdata_{i}.json\")\n",
    "    d = process_Di(mt,_i=i,data=data)\n",
    "    for _k in d.keys():\n",
    "        out_df[_k] = d[_k]\n",
    "    out_df.to_csv(\"eval_reasult2\\\\basellm_score.csv\")\n",
    "    time.sleep(10)\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PK_LLM_endfront.evaluation.eva_funcs import DataLoader\n",
    "import time\n",
    "mt = ['glm-4']#'glm-4'\n",
    "\n",
    "row_df = pd.DataFrame([])\n",
    "for i in range(1,7):\n",
    "    \n",
    "    if i <1:\n",
    "        continue\n",
    "    data = DataLoader.load_json(f\"_data\\\\testqa2\\\\testdata_{i}.json\")\n",
    "    d = process_Di(mt,_i=i,data=data)\n",
    "    for _k in d.keys():\n",
    "        row_df[_k] = d[_k]\n",
    "    time.sleep(10)\n",
    "row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_df.index  = mt\n",
    "df = pd.concat([out_df,row_df],axis=0)\n",
    "# out_df.to_csv(\"eval_reasult2\\\\basellm_score.csv\",index_col=0)\n",
    "new_order = [f'AC{i}' for i in range(1,7)] +[f\"AR{i}\" for i in range(1,7)]\n",
    "df = df[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"eval_reasult2\\\\basellm_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查评测数据中是否存在因为网络问题产生的缺失\n",
    "from PK_LLM_endfront.evaluation.eva_funcs import DataLoader\n",
    "import pandas as pd\n",
    "mt = ['gpt-4','gpt-3.5-turbo','glm-3-turbo','rag','glm-4']#'\n",
    "def check(df,data):\n",
    "    flag = False\n",
    "    for _ in data:\n",
    "        q = _['Question']\n",
    "        if q not in list(df['Question']):\n",
    "            print(q)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def remake_df(df:pd.DataFrame,data):\n",
    "    r_df = pd.DataFrame([])\n",
    "    for _ in data:\n",
    "        q = _['Question']\n",
    "        _df = df[df['Question']==q]\n",
    "        _df = _df.iloc[-1,:]\n",
    "        r_df = pd.concat([r_df,_df],axis=1)\n",
    "    return r_df.T\n",
    "\n",
    "# 检查因为未知原因出现的问答重复问题\n",
    "for i in range(1,7):\n",
    "    dataloader = DataLoader(f\"_data\\\\testqa\\\\testdata_{i}.json\")\n",
    "    data = dataloader.load_json(dataloader.filepath)\n",
    "    for _ in mt:\n",
    "        _p = f\"eval_reasult\\\\{i}/eval_{_}.csv\"\n",
    "        if _ == 'rag':\n",
    "            _p = f\"eval_reasult\\\\{i}\\\\new_eval_rag.csv\"\n",
    "        df = pd.read_csv(_p)\n",
    "        if df.shape[0] != 50:\n",
    "            print(f\"{_} 的第{i}次数据\",df.shape)\n",
    "            #df.to_csv(f\"eval_reasult\\\\{i}/eval_{_}_copy.csv\")\n",
    "            #r_df = remake_df(df,data)\n",
    "            #print(r_df.shape)\n",
    "            #r_df.to_csv(_p)\n",
    "            #if check(df,data):\n",
    "            #    print(f\"{_} 的第{i}次数据,有缺陷\")\n",
    "        has_missing_values = df['prediction'].isnull().any()\n",
    "        if has_missing_values:\n",
    "            print(f\"{_} 的第{i}次数据存在缺失 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef remake_df(df:pd.DataFrame,data):\\n    r_df = pd.DataFrame([])\\n    for _ in data:\\n        q = _[\\'Question\\']\\n        _df = df[df[\\'Question\\']==q]\\n        _df = _df.iloc[-1,:]\\n        r_df = pd.concat([r_df,_df],axis=1)\\n    return r_df.T\\ndf =pd.read_csv( f\"eval_reasult\\\\{1}/eval_glm-3-turbo.csv\")\\ndata = dataloader.load_json(f\"_data\\\\testqa\\\\testdata_{1}.json\")\\nr = remake_df(df,data)\\nr.shape\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def remake_df(df:pd.DataFrame,data):\n",
    "    r_df = pd.DataFrame([])\n",
    "    for _ in data:\n",
    "        q = _['Question']\n",
    "        _df = df[df['Question']==q]\n",
    "        _df = _df.iloc[-1,:]\n",
    "        r_df = pd.concat([r_df,_df],axis=1)\n",
    "    return r_df.T\n",
    "df =pd.read_csv( f\"eval_reasult\\\\{1}/eval_glm-3-turbo.csv\")\n",
    "data = dataloader.load_json(f\"_data\\\\testqa\\\\testdata_{1}.json\")\n",
    "r = remake_df(df,data)\n",
    "r.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"eval_reasult\\\\{1}/eval_glm-4.csv\")\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Unnamed: 0.6          55.0\n",
      "Unnamed: 0.5          55.0\n",
      "Unnamed: 0.4          55.0\n",
      "Unnamed: 0.3          55.0\n",
      "Unnamed: 0.2          55.0\n",
      "Unnamed: 0.1          55.0\n",
      "Unnamed: 0            55.0\n",
      "data                   NaN\n",
      "prediction             NaN\n",
      "Question               NaN\n",
      "answer_similarity      NaN\n",
      "answer_correctness     NaN\n",
      "Name: 0, dtype: float64\n",
      "(50, 12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check(df,data):\n",
    "    flag = False\n",
    "    i=0\n",
    "    for _ in data:\n",
    "        q = _['Question']\n",
    "        if q not in list(df['Question']):\n",
    "            print(q)\n",
    "            flag =  True\n",
    "    return flag\n",
    "\n",
    "for i in range(1,7):\n",
    "    if i != 1:\n",
    "        continue\n",
    "    print(i)\n",
    "    data = DataLoader.load_json(f\"_data\\\\testqa\\\\testdata_{i}.json\")\n",
    "    df = pd.read_csv(f\"eval_reasult\\\\{i}/eval_glm-4.csv\")\n",
    "    print(df.iloc[0,:])\n",
    "    df2 = pd.read_csv(f\"eval_reasult\\\\{i}/eval_glm-3-turbo.csv\")\n",
    "    #if df.shape[0] == 50:\n",
    "    #    continue\n",
    "    r_df = pd.DataFrame([])\n",
    "    for _ in data:\n",
    "        q = _['Question']\n",
    "        _df = df[df['Question']==q]\n",
    "        if _df.shape[0] ==0 or _df['prediction'].isnull().any():\n",
    "            _df = df2[df2['Question']==q]\n",
    "        _df_ = df.iloc[-1,:]\n",
    "        r_df = pd.concat([r_df,_df_],axis=1)\n",
    "    print(r_df.T.shape)\n",
    "    df.to_csv(f\"eval_reasult\\\\{i}/eval_glm-4_copy.csv\")\n",
    "    r_df.T.to_csv(f\"eval_reasult\\\\{i}/eval_glm-4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remake_df(df:pd.DataFrame,data):\n",
    "    r_df = pd.DataFrame([])\n",
    "    for _ in data:\n",
    "        q = _['Question']\n",
    "        _df = df[df['Question']==q]\n",
    "        _df = _df.iloc[-1,:]\n",
    "        r_df = pd.concat([r_df,_df],axis=1)\n",
    "    return r_df.T\n",
    "rdf = remake_df(df2,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AC1</th>\n",
       "      <th>AC2</th>\n",
       "      <th>AC3</th>\n",
       "      <th>AC4</th>\n",
       "      <th>AC5</th>\n",
       "      <th>AC6</th>\n",
       "      <th>AR1</th>\n",
       "      <th>AR2</th>\n",
       "      <th>AR3</th>\n",
       "      <th>AR4</th>\n",
       "      <th>AR5</th>\n",
       "      <th>AR6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>0.683</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>0.684</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glm-3-turbo</th>\n",
       "      <td>0.676</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glm-4</th>\n",
       "      <td>0.702</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AC1    AC2    AC3    AC4    AC5    AC6    AR1    AR2    AR3  \\\n",
       "gpt-4          0.683  0.688  0.737  0.723  0.703  0.711  0.965  0.962  0.962   \n",
       "gpt-3.5-turbo  0.684  0.706  0.714  0.717  0.717  0.760  0.961  0.962  0.953   \n",
       "glm-3-turbo    0.676  0.671  0.670  0.724  0.662  0.685  0.836  0.851  0.871   \n",
       "glm-4          0.702  0.714  0.675  0.667  0.663  0.717  0.953  0.921  0.924   \n",
       "\n",
       "                 AR4    AR5    AR6  \n",
       "gpt-4          0.937  0.913  0.925  \n",
       "gpt-3.5-turbo  0.949  0.933  0.933  \n",
       "glm-3-turbo    0.872  0.773  0.811  \n",
       "glm-4          0.897  0.863  0.905  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"eval_reasult\\\\basellm_score.csv\",index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag 的平均正确率为 0.6334727763571913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d522ec447ea948cda2f70cdbdefb735b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag 的平均相关性为 0.981276611393622\n",
      "rag 的平均正确率为 0.5390943336555015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dec0d2c2804be1b9b3161de85454ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag 的平均相关性为 0.7398016257310778\n",
      "rag 的平均正确率为 0.47858869667478093\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481edd28673d4586b5378f01a76358f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag 的平均相关性为 0.6076840356253885\n",
      "rag 的平均正确率为 0.45921177574420613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d204de7ababc462182ce797042c2248d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag 的平均相关性为 0.5890690008951945\n",
      "rag 的平均正确率为 0.42573479674141423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18def1c79c6c4adb91122a26abc72a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag 的平均相关性为 0.5009171375999143\n",
      "rag 的平均正确率为 0.427610061819362\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6661f2a7e54e6ea307308217e445e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag 的平均相关性为 0.512975632121791\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,7):\n",
    "    d = process_Di(['rag'],i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
