{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('E:\\学习\\python\\py_codbase\\PK_LLM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在可以安全地使用环境变量了\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from PK_LLM_endfront.QA_generation.chains import make_Q_chain,make_RAG_chain,check_QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](P\\whiteboard_exported_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Q(t:str,a:str)->list:\n",
    "    QA_Gen_Chain = make_Q_chain()\n",
    "    r= QA_Gen_Chain.invoke({\n",
    "        \"title\":t,\n",
    "        \"abstract\":a\n",
    "    })\n",
    "    return r.Question\n",
    "def generate_A(pdf_path:str,q_lis:list[str])->dict:\n",
    "    rag_chain = make_RAG_chain(pdf_path) # 有成本\n",
    "    dir = []\n",
    "    for _q in q_lis:\n",
    "        answer = rag_chain.invoke(_q)\n",
    "        #print(\"问题为:\\n\",_q,\"\\n\\n \")\n",
    "        #print(\"\\n答案为:\\n\",answer.Answer,\"\\n\\n \")\n",
    "        #print(\"\\n答案来源为:\\n\",answer.Source_context,\"\\n\\n \")\n",
    "        #print(\"---------------------------\")\n",
    "        dir.append({\"Question\":_q,\"Answer\":answer.Answer,\"Source_context\":answer.Source_context})\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\npath = \"_data/1_Analytical_Methods\"\\ndata = pd.read_csv(f\"{path}/litera_lis.csv\",index_col=0)\\ndata.head(2)\\nr = generate_Q(data.loc[\\'SL01-2-J\\'][\\'Title\\'],data.loc[\\'SL01-2-J\\'][\\'Abstract\\'])\\na = generate_A(f\\'{path}/literature_PDF/SL01-2-J.pdf\\',r)\\nfrom pprint import pprint\\npprint(a)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试demo\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "path = \"_data/1_Analytical_Methods\"\n",
    "data = pd.read_csv(f\"{path}/litera_lis.csv\",index_col=0)\n",
    "data.head(2)\n",
    "r = generate_Q(data.loc['SL01-2-J']['Title'],data.loc['SL01-2-J']['Abstract'])\n",
    "a = generate_A(f'{path}/literature_PDF/SL01-2-J.pdf',r)\n",
    "from pprint import pprint\n",
    "pprint(a)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class qa():\n",
    "    def __init__(self, q:str, a:str, s:str, doi:str,_id:str,time:str) -> None:\n",
    "        self.Question = q\n",
    "        self.Answer = a\n",
    "        self.Source_context = s\n",
    "        self.DOI = doi\n",
    "        self._id = _id\n",
    "        self.time = time\n",
    "    def add_score(self, score:int):\n",
    "        self.score = score\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'Question': self.Question,\n",
    "            'Answer': self.Answer,\n",
    "            'Source_context': self.Source_context,\n",
    "            'DOI': self.DOI,\n",
    "            'time' :str(self.time),\n",
    "            'Score': getattr(self, 'score', None)  # 如果分数不存在，则返回None\n",
    "        }\n",
    "    @staticmethod\n",
    "    def from_dict(data):\n",
    "        qa_instance = qa(data['Question'], data['Answer'], data['Source_context'], data['DOI'],data['time'])\n",
    "        if 'Score' in data:\n",
    "            qa_instance.add_score(data['Score'])\n",
    "        return qa_instance\n",
    "\n",
    "class recorder():\n",
    "    def __init__(self) -> None:\n",
    "        self.qa = []\n",
    "        self.qa_tem = []\n",
    "        self.good_qa = []\n",
    "    \n",
    "    def create_qa_temp(self, q, a, s, doi,_id,time):\n",
    "        QA = qa(q, a, s, doi,_id,time)\n",
    "        self.qa_tem.append(QA)\n",
    "    \n",
    "    def score_temp_qa(self, score_lis:list[int]):\n",
    "        max_s = 0\n",
    "        max_i = 0\n",
    "        for i, _s in enumerate(score_lis):\n",
    "            self.qa_tem[i].add_score(_s)\n",
    "            if _s > max_s:\n",
    "                max_s = _s\n",
    "                max_i = i\n",
    "        self.qa += self.qa_tem\n",
    "        self.good_qa.append(self.qa_tem[max_i])\n",
    "        self.qa_tem = []\n",
    "\n",
    "    def save_json(self, qa_path=\"qa_data.json\", good_qa_path='good_qa_data.json'):\n",
    "        with open(qa_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump([qa_obj.to_dict() for qa_obj in self.qa], file, ensure_ascii=False, indent=4)\n",
    "        with open(good_qa_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump([qa_obj.to_dict() for qa_obj in self.good_qa], file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def load_json(self, qa_path=\"qa_data.json\", good_qa_path='good_qa_data.json'):\n",
    "        with open(qa_path, 'r', encoding='utf-8') as file:\n",
    "            self.qa = [qa.from_dict(data) for data in json.load(file)]\n",
    "        with open(good_qa_path, 'r', encoding='utf-8') as file:\n",
    "            self.good_qa = [qa.from_dict(data) for data in json.load(file)]\n",
    "\n",
    "# 使用示例\n",
    "# rec = recorder()\n",
    "# rec.load_json('qa_data.json', 'good_qa_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import sys\n",
    "\n",
    "def timeout(seconds, error_message='Function call timed out'):\n",
    "    def decorator(func):\n",
    "        def _handle_timeout():\n",
    "            raise TimeoutError(error_message)\n",
    "\n",
    "        def wrapper(*args, **kwargs):\n",
    "            timer = threading.Timer(seconds, _handle_timeout)\n",
    "            timer.start()\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "            finally:\n",
    "                timer.cancel()\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "# 忽略所有警告\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用超时装饰器\n",
    "@timeout(300)  \n",
    "def process_index(df:pd.DataFrame,_index,_recorder:recorder,_id,_pdf_path):\n",
    "    try:\n",
    "        _recorder.load_json(qa_path=f\"_data/QA/qa_data{_id}.json\", good_qa_path=f'_data/QA/good_qa_data{_id}.json')\n",
    "    except:\n",
    "        pass\n",
    "    if df.loc[_index]['is_download'] !=1:\n",
    "        return \n",
    "    t = df.loc[_index]['Title']\n",
    "    a = df.loc[_index]['Abstract']\n",
    "    doi =  df.loc[_index]['DOI']\n",
    "    time = df.loc[_index]['Publication Year']\n",
    "    q_lis  = generate_Q(t,a)\n",
    "    pdf_path = f\"{_pdf_path}/{_index}.pdf\"\n",
    "    a_dir = generate_A(pdf_path,q_lis)\n",
    "    for _ in a_dir:\n",
    "        _recorder.create_qa_temp(q=_[\"Question\"],\n",
    "                                a=_['Answer'],\n",
    "                                s=_['Source_context'],\n",
    "                                doi=doi,\n",
    "                                _id=_index,\n",
    "                                time=time)\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    r =check_QA(a_dir,llm =ChatOpenAI(model='gpt-4',temperature=0.5))\n",
    "    # print(r)\n",
    "    _recorder.score_temp_qa(r.result)\n",
    "    _recorder.save_json(qa_path=f\"_data/QA/qa_data{_id}.json\", good_qa_path=f'_data/QA/good_qa_data{_id}.json')\n",
    "\n",
    "\n",
    "def QA_G(df:pd.DataFrame,index_parts,_pdf_path,_id = 0):\n",
    "    _recorder = recorder()\n",
    "    for _index in index_parts:\n",
    "        if df.loc[_index].get(\"Abstract\") == \"No abstract available for this PMID.\":\n",
    "            continue\n",
    "        try:\n",
    "            process_index(df,_index,_recorder,_id,_pdf_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {_index}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def main_run(_name,num_index:int,_i:str =\"01\"):\n",
    "    path = f\"_data/{_name}\"\n",
    "    _pdf_path = f\"{path}/literature_PDF\"\n",
    "    df = pd.read_csv(f\"{path}\\litera_lis.csv\",index_col=0)\n",
    "    df = df[df['is_download']==1]\n",
    "    index_parts = np.array_split(df.index, 30)\n",
    "    # print(df.index[155:])\n",
    "    QA_G(df,index_parts[num_index],_pdf_path = _pdf_path,_id=_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [02:34<1:14:53, 154.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [04:11<56:17, 120.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Error processing index SL01-22-J: Function schema2 arguments:\n",
      "\n",
      "{\"Answer\":\"Brodifacoum-tainted synthetic cannabinoids impact coagulopathy in patients by causing severe life-threatening bleeding and asymptomatic laboratory coagulopathy. Patients affected by synthetic cannabinoids often have no prior anticoagulation use or rodenticide exposure but may admit to recent inhalation or ingestion of synthetic marijuana. The clinical manifestations of coagulopathy range from asymptomatic laboratory abnormalities to severe bleeding. Laboratory evaluation involves documenting vitamin K-dependent factor deficiencies and detecting long-acting anticoagulant rodenticides (LAARs) through specialized testing. Treatment includes prompt reversal of severe coagulopathy with 4-factor prothrombin complex concentrates and IV vitamin K1, along with extended monitoring of INR and prolonged oral or subcutaneous vitamin K1 therapy until circulating drug levels become undetectable. Monitoring brodifacoum levels can guide the duration of vitamin K1 therapy.\",\"Source_context\":\"Recent multistate outbreaks of coagulopathy caused by brodifacoum-tainted synthetic cannabinoids or \\\\\"                                                                                                    \n",
      "\n",
      "are not valid JSON. Received JSONDecodeError Expecting ',' delimiter: line 1 column 1204 (char 1203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [06:07<53:19, 118.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [08:12<52:32, 121.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [10:28<52:43, 126.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [12:49<52:35, 131.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [14:59<50:12, 131.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [17:07<47:34, 129.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [20:02<50:26, 144.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [22:24<47:45, 143.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [25:06<47:11, 149.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [26:56<41:11, 137.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [29:07<38:19, 135.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [31:47<38:04, 142.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [33:59<34:52, 139.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [36:10<31:57, 136.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [38:51<31:14, 144.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [41:04<28:09, 140.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [43:49<27:10, 148.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [47:08<27:14, 163.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [50:05<25:05, 167.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [52:24<21:11, 158.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [54:33<17:29, 149.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [56:49<14:35, 145.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [58:13<10:36, 127.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [1:00:28<08:38, 129.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [1:02:12<06:05, 121.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [1:03:48<03:48, 114.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [1:05:47<01:55, 115.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:08:05<00:00, 136.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "name = [\"1_Analytical_Methods\",\n",
    "        \"2_Environmenta_Exposure\",\n",
    "        \"3_Environmental_Behavior\",\n",
    "        \"4_Biological_Behavior\",\n",
    "        \"5_Toxicity\",\n",
    "        \"6_Health Risk\"]\n",
    "\n",
    "for i in tqdm(range(30)):\n",
    "    print(i)\n",
    "    if i < 0:\n",
    "        continue\n",
    "    if i <1:\n",
    "        _id = \"0\"+str(i)\n",
    "    else:\n",
    "        _id = str(i)\n",
    "    # main_run(name[5],num_index=i,_i=_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成结束，开始整理并且可视化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PK_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
